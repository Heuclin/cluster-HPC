



![](Figures/logos.png)

Tuto : Cluster Muse
===================

***Axe transversal TIM, UR AIDA***

*Benjamin Heuclin, Ing√©nieur statisticien, UR AIDA, Cirad*

*Septembre 2022*

Licence : <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Licence Creative Commons" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />Ce(tte) ≈ìuvre est mise √† disposition selon les termes de la <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Licence Creative Commons Attribution - Pas d‚ÄôUtilisation Commerciale 4.0 International</a>.

___

0. [C'est quoi un cluster de calcul ? et la parall√©lisation ?](#cluster_parallelisation)
    1. [Cluster de calcul](#cluster)
    2. [La paral√©lisation](#parallelisation)
1. [Connexion au Cluster](#connexion)
2. [Transfer de fichier](#transfer) 
    1. [FileZilla](#filezilla)
    2. [rsync](#rsync)
3. [Les espaces de stockage](#stockage)
4. [Procedure de soumission de jobs R](#proc_soumission)
    1. [Les partitions](#partitions)
    2. [Soumission de jobs](#soumission)
5. [Les commandes SLURM utiles](#commandes)
6. [Exemples](#exemples)
    1. [Exemple R OpenMP](#ex_R_openMP)
    2. [Exemple R array](#ex_R_array)
7. [Version Rstudio en ligne](#rstudio) 
8. [Ressources](#ressources)

___









> üö® Ce document se limite √† l'utilisation de R sur CPU, avec une parall√©lisation en m√©moire partag√©e (openMP). C'est une initation pour prendre rapidement en main le processus de soumission de jobs.

Pour les agents Cirad, il faut demander l'acc√®s √† Bertrand Pitollat ([bertrand.pitollat@cirad.fr](mailto:bertrand.pitollat@cirad.fr)) en lui envoyant un email en pr√©cisant l'unit√©, votre num√©ro de poste t√©l√©phonique CIRAD accompagn√© de la charte sign√©e.  


Vous obtenez  ainsi un nom d'utilisateur (g√©n√©ralement celui de votre compte Cirad) et un mot de passe (g√©n√©ralement le m√™me que celui de votre compte Cirad).

Pour utiliser le cluster, il faut un terminal unix pour ce connect, soumettre et g√©rer vos jobs et un logiciel de transfert de fichier pour envoyer vos codes, ... de votre PC vers le cluster et vice-versa.


Documentation Muse : https://meso-lr.umontpellier.fr/documentation-utilisateurs/


---


<a name="cluster_parallelisation"></a>

# 0. C'est quoi un cluster de calcul ? et la parall√©lisation ?







<a name="cluster"></a>

## 0.1 Cluster de calcul







<a name="parallelisation"></a>

## 0.2 La paral√©lisation

Calcul en m√©moire distribu√© (MPI)

Calcul en m√©moire partag√©e (Multi-thread,
OpenMP)


https://cwant.github.io/hpc-beyond/21-introduction-to-parallelism/index.html


https://stackoverflow.com/questions/32464084/what-are-the-differences-between-mpi-and-openmp


OpenMP is a way to program on shared memory devices. This means that the parallelism occurs where every parallel thread has access to all of your data.

You can think of it as: parallelism can happen during execution of a specific for loop by splitting up the loop among the different threads.

MPI is a way to program on distributed memory devices. This means that the parallelism occurs where every parallel process is working in its own memory space in isolation from the others.

You can think of it as: every bit of code you've written is executed independently by every process. The parallelism occurs because you tell each process exactly which part of the global problem they should be working on based entirely on their process ID.

The way in which you write an OpenMP and MPI program, of course, is also very different.








---


<a name="connexion"></a>

# 1. Connexion au Cluster 


C'est tr√®s simple ! La connexion au cluster de calcul haute performance se fait via le protocole SSH. Le nom d‚Äôh√¥te de la machine de connexion est `muse-login.meso.umontpellier.fr`.

Suivant votre syst√®me d‚Äôexploitation, vous pouvez vous y connecter comme suit :

**Sous linux ou Mac :**

Ouvrir une connexion ssh dans un terminal en tapant la commande suivante : 

```
ssh ¬´nom_utilisateur¬ª@muse-login.meso.umontpellier.fr
```
Entrer ensuite votre MDP.

Sous Mac, vous pouvez √©galement utiliser le logiciel Xquartz.

Vous voil√† maintenant connect√© au cluster Muse. Le cluster Muse utilise le gestionnaire de job SLURM. C'est d'ici que vous pourez ex√©cuter et g√©rer vos jobs avec les commandes sp√©cifiques SLURM (voir plus bas pour les principales commandes). 



**Sous windows :**

Installer le logiciel MobXterm (https://mobaxterm.mobatek.net/download-home-edition.html). Lors de la premi√®re connexion, il faut la configurer !

Configuration :

1. Cliquer sur le bouton Session (en haut √† gauche)
2. Une fen√™tre "*Session settings*" s'ouvre alors
3. Cliquer sur SSH (en haut √† gauche)
4. Remplir les champs suivant : 
    a. *Remote host* : `muse-login.meso.umontpellier.fr`
    b. S√©lectionner *Specify username*
    c. Entrer votre nom d'utilisateur
    d. *Port* : 22
5. Cliquer sur OK


![](Figures/MobaXterm1-2.png)


6. Un terminal unix s'ouvre. 
7. Il faut ensuite entrer votre mot de passe (rien ne s'affiche lorsque vous tapez le mdp, c'est un r√©glage de s√©curit√©) puis valider en appuyant sur "entr√©e".
8. MobaXterm vous demande si vous voulez enregistrer le mdp pour ne plus vous le demander. C'est vous qui voyez !

![](Figures/MobaXterm2-2.png)


> ü§© Pour les prochaines fois, vous n'aurez qu'√† ouvrir MobaXterm et √† cliquer sur votre session que vous trouverez dans l'onglet "*User sessions*" sur la gauche.  
Vous pouvez √©galement cr√©er un raccourci sur votre bureau en faisant un clique droit dessus. Cela permet d'ouvrir votre session en m√™me temps que le logiciel se lance. C'est trop bien ü§©



Vous voil√† maintenant connect√© au cluster Muse. Le cluster Muse utilise le gestionnaire de job SLURM.  C'est d'ici que vous pourrez ex√©cuter et g√©rer vos jobs avec les commandes sp√©cifiques SLURM (voir section [4.2 Soumission de jobs](#soumission)). 








**Quelsques commandes Linux utiles :**

* `ls` pour afficher le contenu du r√©pertoir courant
* `ls -a` pour afficher tous les fichiers (m√™me cach√©) du r√©pertoir courant
* `cd "path"` pour changer de r√©pertoir
* `cd ..` pour aller au r√©pertoir parent
* `pwd` pour afficher le chemin absolut du r√©pertoir courant (depuis la racine)
* ‚¨ÜÔ∏è‚¨áÔ∏è **Fl√®che haut/bas** pour naviger dans historique des commandes utilis√©es

Pour plus d'info sur les commandes Linux de bases :
https://doc.ubuntu-fr.org/tutoriel/console_commandes_de_base







<a name="transfer"></a>

# 2. Transfer de fichiers

Pour soumettre vos jobs, il va falloir envoyer vos scripts sur le cluster. Il vous faudra ensuite r√©cup√©rer les fichiers g√©n√©r√©s par vos jobs. Pour ce faire, on va utiliser le logiciel FileZilla. Il est disponible sous Windows, OSX et Linux. Pour le t√©l√©chargement de fichiers volumineux du cluster vers votre machine (long avec FileZilla) il est possible d'utiliser "rsunc"


<a name="filezilla"></a>

# 2.1. FileZilla


Installer FileZilla : https://filezilla-project.org/download.php?show_all=1

> **Remarque pour Linux** : Filezilla est disponible par l‚Äôinterm√©diaire de votre Gestionnaire de paquets `apt-get install filezilla`

**Pr√©sentation de FileZilla : **

![](Figures/Filezilla2.png)

Pour ce connecter, remplir dans la zone de connection :

* **H√¥te** : `sftp://muse-login.meso.umontpellier.fr`
* **Nom d'utilisateur** : votre nom d'utilisateur 
* **Mot de passe** : votre mot de passe
* **Port** : 22


> ü§© Apr√®s la premi√®re connexion, ces informations seront enregistr√©es et vous pourrez vous connecter facilement en cliquant sur la petite fl√®che √† c√¥t√© de "Connexion rapide"

Vous pouvez transf√©rer un fichier dans un sens ou dans l'autre en cliquant droit dessus puis cliquer sur "T√©l√©vers√©" ou "T√©l√©charger".







<a name="rsunc"></a>

# 2.2. rsync

source : https://meso-lr.umontpellier.fr/documentation-utilisateurs/

```
#! /usr/bin/env bash
###################################################################
# rsync.sh : Ecrit par J√©r√©my Verrier				  #
# Script permettant la copie s√©curis√©e de fichiers ou de dossiers #
###################################################################

# Entrez votre nom d'utilisateur
USER=
# Entrez le chemin complet du r√©pertoire ou du fichier √† copier (/home/verrier/work/results.txt)
DOSSIER_CLUSTER=
# Entrez le chemin complet du r√©pertoire ou du fichier de destination
DOSSIER_PERSO=

while [ 1 ]
do
    rsync -avz --progress --partial "${USER}"@muse-login.meso.umontpellier.fr:"${DOSSIER_CLUSTER}" "${DOSSIER_PERSO}"
    if [ "$?" = "0" ] ; then
        echo "Rsync OK"
        exit
    else
        echo "Rsync erreur, nouvelle tentative dans 1 minute..."
        sleep 60
    fi
done
```


A enregister au au format .sh ou √† t√©l√©charger avec le lien ci-dessous.

[Ce script](https://hpc-lr.umontpellier.fr/wp-content/uploads/2017/05/rsync.txt) vous permet de copier des donn√©es depuis le cluster Muse vers votre machine.
Il vous faut modifier les champs USER, DOSSIER_CLUSTER et DOSSIER_PERSO et ensuite le lancer avec la commande ¬´ bash rsync ¬ª.
Il est vivement conseill√© d‚Äôutiliser ce script lors de t√©l√©chargement de fichiers volumineux.





<a name="stockage"></a>

# 3. Les espaces de stockage

Il y a plusieurs espaces de stockage. Les fichiers d√©pos√©s sur le r√©pertoire "scratch" sont temporaires pour effectuer vos calculs, et sont automatiquement supprim√©s √† 60 jours. Les documents destin√©s √† √™tre conserv√©s doivent √™tre d√©pos√©s sur votre r√©pertoire "home".

Email de Bertrand Pitollat du 19/10/2021 :

___
Vous disposez de plusieurs espaces de stockage sur le cluster Muse :

- **home directory** : C'est votre point d'entr√©e sur le cluster Muse.
  * Il est h√©berg√© sur la baie NFS du cluster Muse.
  * Il n'est ni sauvegard√© ni r√©pliqu√©.
  * Il est limit√© √† un quota de 50 Go (hors autres espaces de stockage).
  * Il est accessible en lecture et en √©criture depuis les noeuds de login et en lecture seule depuis les noeuds de calcul.

- **r√©pertoire personnel sur la baie r√©pliqu√©e : lien replicated**
  * Il est h√©berg√© sur la baie NetApp du cluster Muse.
  * Ce r√©pertoire est personnel.
  * Il est destin√© au stockage long terme des donn√©es personnelles.
  * Il est accessible via le lien replicated de votre home directory (par exemple, /home/pitollatb/replicated => /storage/replicated/cirad_users/pitollatb).
  * Il est sauvegard√© et r√©pliqu√© sur une baie de secours.
  * Il est limit√© √† un quota de 500Go.
  * Il est accessible en lecture et en √©criture depuis les noeuds de login et de calcul.

- **espace projets / unit√©s / √©quipes sur la baie r√©pliqu√©e : lien projects**
  * Cet espace est h√©berg√© sur la baie NetApp du cluster Muse.
  * Il est destin√© au stockage long terme des donn√©es projets / unit√©s / √©quipes.
  * L'espace est accessible via le lien projects de votre home directory (par exemple, /home/pitollatb/projects => /storage/replicated/cirad/projects).
  * Il est sauvegard√© et r√©pliqu√© sur une baie de secours.
  * Il est partitionn√© par r√©pertoire projet, unit√© ou √©quipe avec un quota initial de 5To pour chaque r√©pertoire.
  * Chaque r√©pertoire projet peut √™tre associ√© √† un groupe d'utilisateurs √† d√©finir par le collectif.
  * L'espace est accessible en lecture et en √©criture depuis les noeuds de login et de calcul.

- **espace work : lien work_agap**
  * Il est h√©berg√© sur la baie NFS du cluster Muse.
  * Cet espace de stockage pr√©c√©demment d√©di√© au stockage des donn√©es projets ne doit plus √™tre utilis√©.
  * Les donn√©es s'y trouvant doivent √™tre transf√©r√©es dans le nouvel espace projets / unit√©s / √©quipes.
  * Il n'est ni sauvegard√© ni r√©pliqu√©.
  * Il est accessible via le lien work_agap de votre home directory (par exemple, /home/pitollatb/work_agap => /nfs/work/agap).
  * Il est accessible en lecture et en √©criture depuis les noeuds de login et en lecture seule depuis les noeuds de calcul.

- **espace personnel scratch : lien scratch**
  * Il est h√©berg√© sur la baie Lustre du cluster Muse.
Ce espace est personnel.
  * Il est rapide et performant et doit √™tre utilis√© pour h√©berger les donn√©es temporaires de calcul.
  * A la fin du calcul, les donn√©es doivent √™tre supprim√©es ou d√©plac√©es.
  * Il est accessible via le lien scratch de votre home directory (par exemple, /home/pitollatb/scratch => /lustre/pitollatb).
  * Il n'est ni sauvegard√© ni r√©pliqu√©.
  * Il est limit√© dans le temps : les donn√©es vieilles de plus de 2 mois seront bient√¥t automatiquement supprim√©es.
  * Il est accessible en lecture et en √©criture depuis les noeuds de login et de calcul.

- **banques de donn√©es scratch : /lustre/agap**
  * Cet espace communautaire stocke les banques de donn√©es.
  * Il est h√©berg√© sur la baie Lustre du cluster Muse pour optimiser les calculs.
  * Il n'est ni sauvegard√© ni r√©pliqu√©.
  * Il ne doit pas √™tre utilis√© pour stocker de donn√©es personnelles.
  * Il est accessible √† l'emplacement /lustre/agap.
  * Les banques maintenues par biomaj sont accessibles √† l'emplacement /lustre/agap/BANK/biomaj.

- **espace web :** Par ailleurs, il existe un espace d√©di√© pour h√©berger les donn√©es affich√©es/diffus√©es par nos diff√©rents services web (genome hubs, ...).
Nous contacter si n√©cessaire.

**Important :**
Je vous rappelle que pour des raisons de performance du cluster Muse, toutes les √©critures issues des jobs doivent √™tre redirig√©es vers votre r√©pertoire scratch et qu'il faut bannir toute lecture intensive depuis les espaces NFS et NetApp.

___






<a name="proc_soumission"></a>

# 4. Procedure de soumission de jobs R

Dans cette section j'explique comment soumettre des jobs en parall√®le sous R. Avant de rentrer dans le vif du sujet, il faut choisir une partition sur laquelle lancer les jobs.


<a name="partitions"></a>

## 4.1. Les partitions

Il faut choisir le type de partition sur laquelle lancer vos jobs. Il existe plusieurs partions avec des param√®tres diff√©rents sur lesquelles vous pouvez soumettre vos jobs :

![](Figures/partition.PNG)

Sur chaque partition, il y a un certain nombre de noeuds (nodes) (je n'ai pas compt√© combien). Chaque noeud contient 28 coeurs (cores).
Pour les novices, un noeud peut √™tre apparent√© √† un ordinateur et les coeurs aux processeurs.

En tant qu'utilisateur Cirad, nous avons acc√®s aux partitions :

* *agap_short* pour des jobs rapides (limite de 1h)
* *agap_normal* pour des jobs de 2 jours max
* *agap_long* pour des jobs chronophages (pas de limite de temps)
* *agap_bigmem* pour des jobs n√©cessitant beaucoup de m√©moire vive (pas de limite de temps)

La m√©moire vive par coeur est limit√©e par d√©faut (voir 3√®me colonne) mais elle peut √™tre augment√©e en ajoutant le param√®tre `--mem-per-cpu=XG`, avec X la quantit√© de m√©moire allou√©e au job (en th√©orie, max de 128G pour la queue *agap_short*, *agap_normal*, et *agap_long*, et max de 3000G pour la queue *agap_bigmem*).








<a name="soumission"></a>

## 4.2. Soumission de jobs


Pour illustrer la soumission d'un job en parall√®le, nous utiliserons l'Exemple_R. Dans cet exemple bidon, je r√©p√®te l'op√©ration 2*k pour k=1 √† 50. Je veux parall√©liser ces op√©rations sur 10 coeurs pour aller 10 fois plus vite. Je sauvegarde chaque r√©sultat dans un ".Rdata" dans un fichier "results".


Pour soumettre un job, vous devez choisir entre :

  * Un mode d‚Äôex√©cution en temps r√©el avec la commande srun directement dans le terminal (non d√©taill√© dans ce tuto)
  * Un mode d‚Äôex√©cution diff√©r√© en d√©finissant son job dans un script ***batch*** et en le lancer √† l‚Äôaide de la commande `sbatch` dans le terminal


‚ö†Ô∏è **J'explique ici uniquement la proc√©dure diff√©r√©e avec la commande `sbatch`** 

Cette proc√©dure consiste √† d√©finir les param√®tres d'ex√©cution dans un fichier *batch* (.sh)

Ce fichier peut √™tre √©diter avec [**Notepad++**](https://notepad-plus-plus.org/downloads/) ou **Rstudio**.
Pour le cr√©er : 

* avec **Notepad++** : *File > New* puis sauvegarder avec l'extension *.sh*
* avec **Rstudio** : *File > New File > Shell Script*

üëâ Mais le plus simple est de reprendre un fichier .sh qu'on a sous la main 



üí•üî• **Alerte Windows** üö®üßØ **Attention aux retours √† la ligne !!!**  
Par d√©fault dans Windows les retours chariot sont de type DOS et ils ne sont pas compatible avec Linux ou Max de type UNIX (Posix LF) et √ßa plantle !  
Il faut soit :

* utiliser **Notepad++** en faisant :  
*Edition > Convertir les sauts de ligne > Convertir en format UNIX (LF)*  
‚ö†Ô∏è a faire pour chaque nouveau fichier
* utiliser **Rstudio** en r√©glant l'option des retours √† la ligne de type Unix :  
*Tools > Global options > Code > Saving > Serialization > Line ending conversion > Posix (LF)*  
A faire qu'une seule fois ‚ù§Ô∏èüí™ Il sait tout faire ce Rstudio ! üí™‚ù§Ô∏è 





**Prenons le fichier *batch* de l'Exemple_R** pour voir sa construction

```
#!/bin/bash
#SBATCH --partition=agap_short
#SBATCH --job-name ex1
#SBATCH --nodes=1
#SBATCH --ntasks=10
#SBATCH --mem-per-cpu=1G
#SBATCH --time=01:00:00
#SBATCH --mail-type=begin        # send email when job begins
#SBATCH --mail-type=end          # send email when job ends
#SBATCH --mail-user=benjamin.heuclin@cirad.fr

module purge              # d√©charge tous les modules       
module load cv-standard   # majorit√© des compilateurs/biblioth√®ques standards
module load R/3.6.1       # chargement de R version 3.6.1

R CMD BATCH /storage/replicated/cirad/projects/AIDA/Atelier_cluster/Exemple_1/main_script.R    /storage/replicated/cirad/projects/AIDA/Atelier_cluster/Exemple_1/Rout/main_script.Rout

```

Ce fichier se d√©compose en 3 parties :

* 1re partie : chaque ligne commen√ßant par `#SBATCH` d√©crit un param√®tre **SLURM** :
    * `--partition=` pour s√©lectionner la partition
    * `--job-name ` pour donner un nom √† la soumission
    * `--nodes=` nombre de noeuds (toujours 1 dans ce tuto)
    * `--ntasks=` nombre de t√¢ches pour la parall√©lisation, correspond au nb de coeurs puisque 1 noeud 
    * `--mem-per-cpu=` quantit√© de m√©moire vive par coeur
    * `--time=` temps maximum
    
D'autres options existent, se r√©f√©rer √† la documentation du cluster pour plus d'info (https://meso-lr.umontpellier.fr/documentation-utilisateurs/).

* 2√®me partie : Il faut ensuite charger les **modules** (logiciels, compilateurs) avec la commande `module load`. 
[Les modules reposent sur un syst√®me de d√©pendances et de conflits fix√©s par la personne ayant install√© ou compil√© le logiciel ou la librairie vis√©e.]: # 
On commance par d√©charcher tous les modules qui peuvent √™tre charg√©s.
Pour utiliser le logiciel R, en fonction de la version, il faut charger le module `cv-standard` et ensuite `R/version` ou `R` (charge la derni√®re version 4.2.2). Si votre code utilise un autre langage (c++, python, ...), il faudra alors charger les modules en cons√©quence. Pour voir la liste des modules disponibles : `module avail`. Plus d'info sur les modules dans la documentation du cluster (https://meso-lr.umontpellier.fr/documentation-utilisateurs/) section "Environnement logiciel du Cluster Muse" et le TP : https://meso-lr.umontpellier.fr/wp-content/uploads/2020/03/1-TP-Environment_module3.pdf. Je vous conseille de pr√©cicer la version de R avec laquelle vous voulez travailler pour ne pas avoir de surprise si une nouvelle version est install√©e. Les versions de R disponibles sont :
    * en local (install√© par muse@lr) :
        * `R/3.6.1`
        * `R/3.6.1-tcltk `
        * `R/3.6.3 `
        * `R/4.0.2`
        * `R/4.2.2`
    * dans cv-standard (la majorit√© des compilateurs et biblioth√®ques standards) :
        * `R/3.3.1`
        * `R/3.4.3`
    
**Vous pouvez aussi installer vos propres logiciels.**


* 3√®me partie : Enfin la ligne pour ex√©cuter le script R : `R CMD BATCH ` suivie du chemin d'acc√®s vers le script, suivi du chemin d'acc√®s vers le **Rout** pour √©crire les sorties (ce qui s'affiche dans la console de Rstudio en temps normal). **Pensez √† cr√©er le fichier Rout dans votre projet**.





Pour lancer votre code, il faut ex√©cuter dans le terminal le fichier batch associ√© avec la commande :
```
sbatch job_submission.sh
```




**Attention** : si votre code n√©cessite le chargement de packages ("doParallel" pour l'Exemple_1), il faut imp√©rativement les installer avant. Pour ce faire, dans le terminal, charger les modules puis lancer R :
```
module load cv-standard R/3.6.1
R
```
Installer ensuite les packages (il vous faudra choisir un miroir) :
```
install.packages("doParallel")
```
Enfin quitter R avec la commande `q()`.









**Si tu as un fichier coronpu √† cause des retour √† la ligne**
```
-bash-4.2$ sbatch job_submission.sh
sbatch: error: Batch script contains DOS line breaks (\r\n)
sbatch: error: instead of expected UNIX line breaks (\n).
```
tu peux soit :

* dans **Notepad++** faire :  *Edition > Convertir les sauts de ligne > Convertir en format UNIX (LF)*
* dans **Rstudio** (avec l'option qui va bien comme d√©crit juste au dessus) : modifier l√©g√®rement ton .sh (avec un saut de ligne par exemple) et le sauvegarder. Rstudio va automatiquement convertir les sauts de ligne









<a name="commandes"></a>

# 5. Les commandes SLURM utiles


Pour voir l'√©tat de tous les jobs (de tous les utilisateurs)
```
squeue
```


Pour voir l'√©tat de vos jobs 
```
squeue -u $USER -o '%.18i %.9P %.20j %u %.8T %.10M %.9l %.6D %R'
```


Pour tuer un job
```
scancel <JOB_ID>
```


Pour voir le nombre de coeurs disponibles par noeud (tr√®s utile pour choisir le nombre de coeurs pour passer devant toute la fille d'attente)
```
sinfo -o "%P %n %C"
```

* La 1er colonne (%P) donne la partition
* La 2√®me colonne (%n) donne l'identifiant du noeud
* La 3√®me colonne (%C) donne le nombre de CPUs par √©tat dans le format "allou√©/libre/autre/total"

![](Figures/sinfo.png)



Consulter la quantit√© de m√©moire consomm√©e par le job apr√®s son ex√©cution : 
```
sacct -o JobID,Node,AveRSS,MaxRSS,MaxRSSTask,MaxRSSNode,TRESUsageInTot%250 -j <JOB_ID> 
```



Plus d'info sur les commandes ici : https://slurm.schedmd.com/man_index.html













<a name="exemples"></a>

# 6. Exemples





<a name="ex_R_openMP"></a>

## 6.1. Exemple R OpenMP













<a name="ex_R_array"></a>

## 6.2. Exemple R array

Dans cet exemple, je souhaite lancer un code (une fonction) mais avec plusieurs param√®tres diff√©rents en entr√©e (On peut aussi imaginer lancer un code sur plusieurs jeux de donn√©es diff√©rentes ou mixer les deux).

Pour cela j'ai une fonction `my_fct` qui prend 3 param√®tre d'entr√©e `n`, `p` et `k` et qui renvoie le produit `n*p*k` (tr√®s simple !) et je veux l'appliquer pour diff√©rentes valeurs d'entr√©e :

* `n` = 1 ou 2
* `p` = 1, 2 ou 3
* `k` = 1 ou 2

Cel√† fait `2*3*2 = 12` combinaisons et donc 12 ex√©cutions de mon code. Les ex√©cutions sont ind√©pendantes et donc on peut optimiser le lancement de ce job √† l'aide d'un **array**. Le principe est qu'on demande au cluster un certain nombre de CPUs et le cluster va les choisir potentiellement dans des noeuds (node) differents.   
On est donc sur une forme de paral√©lisation MPI ! 

> Lorsqu'il y a beaucoup de jobs en attente sur le cluster, cela permet √† votre job de passe plus vite car c'est plus facile de prendre *n* CPUs par-ci par-l√† plut√¥t que *n* CPUs sur le m√™me noeud 

Pour ce faire, on sp√©cifie l'option `--array` dans le batch (.sh) :


```
#!/bin/bash
#SBATCH --partition=agap_short  # la partition
#SBATCH --job-name array        # nom du job
#SBATCH --array=1-12            # OPTION ARRAY 
#SBATCH -o array-%a.out
#SBATCH --mem-per-cpu=100M      # M√©moire par CPU
#SBATCH --time=00:30:00         # Temps limite

module purge
module load cv-standard
module load R/3.6.1

cd $SLURM_SUBMIT_DIR

Rscript ./main_script.R $SLURM_ARRAY_TASK_ID
```

‚ö†Ô∏è **Attention** : On ne pr√©cise pas le nombre de noeud (-N) et de coeur (-n) que l'on souhaite. C'est SLURM qui va r√©partir en fonction des ressources disponibles.

Sur la derni√®re ligne d'ex√©cution du script R, on rajoute la variable d'environnement `$SLURM_ARRAY_TASK_ID`, cela permet au script R de r√©cup√©rer le num√©ro de la t√¢che.  
Et enfin dans le script R il faut utiliser la commande `as.numeric(commandArgs(trailingOnly=TRUE)[1])` pour r√©cup√©rer l'indice (*i*). Je peux ainsi lancer la fonction sur la ligne *i* √®me ligne de ma grille de param√®tres.

```
# on r√©cup√®re l'indice de la t√¢che ($SLURM_ARRAY_TASK_ID)
i = as.numeric(commandArgs(trailingOnly=TRUE)[1])

# D√©finition d'une grille de param√®tres que je veux faire varier
pars <-  expand.grid(n = 1:2, p = 1:3, k = 1:2)

# D√©finition de ma fonction
my_fct <- function(n, p, k) return(n*p*k)

# Execution de la fonction sur la ligne i de la grille de param√®tres
result <- my_fct(n=pars$n[i], p=pars$p[i], k=pars$k[i])
print(paste0("Le resultat de ma fonction est : ", result))
```











<a name="rstudio"></a>

# 7. Version Rstudio en ligne 

http://193.52.26.138/rstudio/auth-sign-in

* **Partition** : ???
* **Nb nodes** : ???
* **Nb cores** : ???
* **Limite de temps** : ???
* **M√©moire** : ???




<a name="ressources"></a>

# 8. Ressources

**Mailing list d'entraides :**
meso-help@umontpellier.fr


**Site web sur cluster Muse HPC@LR :**
https://meso-lr.umontpellier.fr/documentation-utilisateurs/


**Pr√©sentation du cluster Muse HPC@LR :**
https://meso-lr.umontpellier.fr/wp-content/uploads/2019/11/1-Presentation_cluster_Muse.pdf

**TP-Environment Module :**
https://meso-lr.umontpellier.fr/wp-content/uploads/2020/03/1-TP-Environment_module3.pdf

**TP-SLURM :**
https://meso-lr.umontpellier.fr/wp-content/uploads/2020/04/1-TP-SLURM3.pdf

**Commandes Linux de bases :**
https://doc.ubuntu-fr.org/tutoriel/console_commandes_de_base

